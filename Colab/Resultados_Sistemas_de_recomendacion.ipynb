{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Sistemas_de_recomendacion.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP1LzmxqnFl2",
        "colab_type": "code",
        "outputId": "3cc5adf6-819b-47d5-a83b-dce1c88c1a61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, col\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext\n",
        "\n",
        "triplets_file = 'https://static.turi.com/datasets/millionsong/10000.txt'\n",
        "song_df_1 = pd.read_table(triplets_file,header=None)\n",
        "song_df_1.columns = ['user_id', 'song_id', 'listen_count']\n",
        "song_df_1.to_csv('tripletas.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8MB 64kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 44.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=61db74e86d14b6d4f7520db3d7ddea8a3f679e5b9cd0c6c4c207bc3189110307\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n",
            "openjdk-8-jdk-headless is already the newest version (8u242-b08-0ubuntu3~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3XbgiTfnYVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T0ShIBu__St",
        "colab_type": "text"
      },
      "source": [
        "# User-User"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgFm175el6Pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import FloatType, ArrayType, StringType\n",
        "triplets_file = 'tripletas.csv'\n",
        "song_df = spark.read.csv(triplets_file, header='true')\n",
        "user_mean = song_df.groupBy('user_id').agg(mean('listen_count').alias(\"media\"))\n",
        "song_mean = song_df.join(user_mean,on='user_id', how='inner')\n",
        "song_mean = song_mean.withColumn('count_mean',col('listen_count')/col('media'))\n",
        "only_jams=song_mean.filter(song_mean.listen_count>song_mean.media).select(['user_id','song_id'])\n",
        "\n",
        "\n",
        "\n",
        "def jaccard_similarity(list1, list2):\n",
        "    intersection = len(list(set(list1).intersection(list2)))\n",
        "    union = (len(list1) + len(list2)) - intersection\n",
        "    return float(intersection) / union\n",
        "\n",
        "\n",
        "def news(s1,s2):\n",
        "    return list(set(s2).difference(set(s1)))\n",
        "\n",
        "\n",
        "user_set_jams = only_jams.groupby(\"user_id\").agg(collect_set(\"song_id\").alias('Set_Songs'))#.limit(100) ### para cada usuario el set de lo que le gusta\n",
        "user_set_jams_2 = user_set_jams.withColumnRenamed('user_id','user_id_2')\n",
        "user_set_jams_2 = user_set_jams_2.withColumnRenamed('Set_Songs','Set_Songs_2')\n",
        "\n",
        "\n",
        "### Usando pre filtro Pre filtro\n",
        "\n",
        "only_jams2 = only_jams.withColumnRenamed('user_id','user_id_2')\n",
        "\n",
        "pre_double_jams = only_jams.join(only_jams2,on='song_id')\n",
        "\n",
        "pre_double_jams = pre_double_jams.filter( pre_double_jams.user_id != pre_double_jams.user_id_2).select('user_id','user_id_2')\n",
        "\n",
        "\n",
        "pre_double_jams = pre_double_jams.groupBy('user_id','user_id_2').agg(count('*').alias('number_common'))\n",
        "\n",
        "pre_double_jams = pre_double_jams.filter(pre_double_jams.number_common>1)\n",
        "\n",
        "pre_double_jams = pre_double_jams.select('user_id','user_id_2')\n",
        "\n",
        "pre_double_jams = pre_double_jams.dropDuplicates()\n",
        "\n",
        "double_jams = pre_double_jams.join(user_set_jams,on='user_id').join(user_set_jams_2,on='user_id_2') ### AQui tenemos un dataframe de 2 usuarios con sus listas de jams\n",
        "\n",
        "\n",
        "\n",
        "### Sin pre filtro\n",
        "#double_jams = user_set_jams.crossJoin(user_set_jams_2)\n",
        "#double_jams = double_jams.filter(double_jams.user_id!=double_jams.user_id_2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "news_udf = udf(lambda y: news(y[0],y[1]), ArrayType(StringType()))\n",
        "\n",
        "jack = udf(lambda x: jaccard_similarity(x[0],x[1]), FloatType())\n",
        "\n",
        "user_jack=double_jams.withColumn('Jackard', jack(array('Set_Songs', 'Set_Songs_2'))).select('user_id','user_id_2','Jackard') ### AQUI calculamos que tan aprecidos son 2 usuarios\n",
        "\n",
        "user_jack=user_jack.filter(user_jack.Jackard>0)\n",
        "\n",
        "window = Window.partitionBy(user_jack['user_id']).orderBy(user_jack['Jackard'].desc())\n",
        "\n",
        "same_user = user_jack.select('*', rank().over(window).alias('rank')).\\\n",
        "                  filter(col('rank') <= 10).select('user_id','user_id_2','Jackard') #### Para cada usuario voy a ver cuales son sus 10 más aprecidos\n",
        "\n",
        "#same_user.toPandas().to_csv('drive/My Drive/Ms Ciencia de datos/user-sim.csv')\n",
        "\n",
        "same_user = same_user.withColumnRenamed('user_id','user_id_new')\n",
        "same_user = same_user.withColumnRenamed('user_id_2','user_id')\n",
        "\n",
        "\n",
        "user_jams = only_jams.groupby(\"user_id\").agg(collect_set(\"song_id\").alias('Set_songs')) ### Para cada usuario veo cual es su lista de jams\n",
        "\n",
        "\n",
        "resultado = only_jams.join(same_user, on = 'user_id').groupby([\"user_id_new\",\"song_id\"]).\\\n",
        "          agg(sum(\"Jackard\").alias('score_recomendacion')).\\\n",
        "          sort(desc(\"score_recomendacion\")).\\\n",
        "          withColumnRenamed('user_id_new','user_id').\\\n",
        "          groupby(\"user_id\").agg(collect_set(\"song_id\").alias('Set_recomendaciones')).\\\n",
        "          join(user_jams, on = 'user_id').\\\n",
        "          withColumn('recomendacion', news_udf(array('Set_Songs', 'Set_recomendaciones'))).\\\n",
        "          select('user_id','recomendacion') ### Para cada usuario se le dan las recomendaciones con base en los 10 usuarios alike segun jaccrd\n",
        "          ### Importante ver que se suman todos los jams de los alikes y al final se pondera cada voto segun que tan parecidos son\n",
        "          ### Finalmente el resultado es ordenado y solo se recomienda lo que no está en el set de jams de la persona\n",
        "\n",
        "\n",
        "%time resultado.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPSjKyRAAD0c",
        "colab_type": "text"
      },
      "source": [
        "# Song-Song"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XD7B4vMu537",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triplets_file = 'tripletas.csv'\n",
        "song_df = spark.read.csv(triplets_file, header='true')\n",
        "user_mean = song_df.groupBy('user_id').agg(mean('listen_count').alias(\"media\"))\n",
        "song_mean = song_df.join(user_mean,on='user_id', how='inner')\n",
        "song_mean = song_mean.withColumn('count_mean',col('listen_count')/col('media'))\n",
        "\n",
        "only_jams=song_mean.filter(song_mean.listen_count>song_mean.media*0.8).select(['user_id','song_id'])\n",
        "\n",
        "jams_set_users = only_jams.groupby(\"song_id\").agg(collect_set(\"user_id\").alias('Set_users'))# Para cada cancion tenemos el grupo de usuarios que le gustan\n",
        "\n",
        "jams_set_users_2 = jams_set_users.withColumnRenamed('song_id','song_id_2')\n",
        "\n",
        "jams_set_users_2 = jams_set_users_2.withColumnRenamed('Set_users','Set_users_2')\n",
        "\n",
        "def jaccard_similarity(list1, list2):\n",
        "    intersection = len(list(set(list1).intersection(list2)))\n",
        "    union = (len(list1) + len(list2)) - intersection\n",
        "    return float(intersection) / union\n",
        "\n",
        "def news(s1,s2):\n",
        "    return list(set(s2).difference(set(s1)))\n",
        "\n",
        "jack = udf(lambda x: jaccard_similarity(x[0],x[1]), FloatType())\n",
        "\n",
        "news_udf = udf(lambda y: news(y[0],y[1]), ArrayType(StringType()))\n",
        "\n",
        "### El siguiente bloque es para solo calcular jaccard entre las canciones que esten el al menos el jamlist de 2 usuarios\n",
        "only_jams2 = only_jams.withColumnRenamed('song_id','song_id_2')\n",
        "\n",
        "double_jams = only_jams.join(only_jams2,on='user_id')\n",
        "\n",
        "double_jams = double_jams.filter(double_jams.song_id!=double_jams.song_id_2)\n",
        "\n",
        "initial_double_songs = double_jams.groupBy('song_id','song_id_2').agg(count('*').alias('number_common'))\n",
        "\n",
        "initial_double_songs = initial_double_songs.filter(initial_double_songs.number_common>1)\n",
        "\n",
        "initial_double_songs = initial_double_songs.select('song_id','song_id_2')\n",
        "\n",
        "#####\n",
        "\n",
        "\n",
        "double_users = initial_double_songs.join(jams_set_users, on = 'song_id').join(jams_set_users_2, on = 'song_id_2')## en el mismo dataframe ya tenemos la lista de usuarios a los que le gusta de cada par de cnaciones que nos interesan\n",
        "\n",
        "double_users = double_users.filter(double_users.song_id!=double_users.song_id_2)\n",
        "\n",
        "initial_rec=double_users.withColumn('Jackard', jack(array('Set_users', 'Set_users_2'))) ### Calculo jackard para las dos canciones\n",
        "\n",
        "initial_rec = initial_rec.filter(initial_rec.Jackard>0) ### Solo me interesan los que le gustan\n",
        "\n",
        "window = Window.partitionBy(initial_rec['song_id']).orderBy(initial_rec['Jackard'].desc())\n",
        "\n",
        "same_song = initial_rec.select('*', rank().over(window).alias('rank')).\\\n",
        "                  filter(col('rank') <= 10).select('song_id','song_id_2','Jackard') ### Para cada cancion me quedo con las 10 más similares a ella\n",
        "\n",
        "#same_song.toPandas().to_csv('drive/My Drive/Ms Ciencia de datos/song-sim.csv')\n",
        "\n",
        "#same_song.toPandas.to_csv('song_song_similarity.csv') # Esto es lo más lento, por eso mejor guardarlo\n",
        "\n",
        "user_jams = only_jams.groupby(\"user_id\").agg(collect_set(\"song_id\").alias('Set_songs')) ### Para cada usuario veo cual es su lista de jams\n",
        "\n",
        "resultado = only_jams.join(same_song,on='song_id').groupby([\"user_id\",\"song_id_2\"]).\\\n",
        "          agg(sum(\"Jackard\").\\\n",
        "          alias('score_recomendacion')).\\\n",
        "          sort(desc(\"score_recomendacion\")).\\\n",
        "          groupby(\"user_id\").agg(collect_set(\"song_id_2\").alias('Set_recomendaciones')).\\\n",
        "          join(user_jams, on = 'user_id').\\\n",
        "          withColumn('recomendacion', news_udf(array('Set_Songs', 'Set_recomendaciones'))).\\\n",
        "          select('user_id','recomendacion') ### segun las canciones que le gustan a cada persona, le recomiendo todo lo que \"es igual\" segun same songs y que no está en su jams list\n",
        "\n",
        "%time resultado.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}